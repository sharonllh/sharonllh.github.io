---
layout: post
title: DDIA阅读笔记(3)：存储与检索
date: 2024-03-04 +0800
tags: [系统设计, DDIA]
categories: [系统设计]
---

## 数据库存储引擎分类

根据访问模式的不同，分为两大类：
- 针对事务处理的存储引擎（OLTP）

  面向最终用户，系统可能会收到大量的请求，但每个查询通常只访问少量的记录。硬盘查找时间是瓶颈，可以使用索引来优化。

- 针对在线分析的存储引擎（OLAP）

  面向业务分析人员，查询量比OLTP少得多，但每个查询需要在短时间内扫描大量（数百万条）记录。硬盘带宽是瓶颈，可以使用列式存储来优化。

OLTP可以进一步分为两类：
- 日志结构的存储引擎
  
  只允许追加到文件和删除过时的文件，而不会更新已经写入的文件。

- 面向页面的存储引擎

  把硬盘看作一组固定大小的页面，页面可以被覆写。

## 日志结构（log-structured）的存储引擎

### 什么是日志结构？

日志：一个仅追加的数据文件。不管是新建、更新还是删除，都只在文件尾部追加写。

在文件尾部追加写是非常高效的，但是查询就不是那么高效了，需要从头到尾扫描整个文件，开销是`O(n)。`

为了提高查询的效率，需要使用索引。

### 什么是索引？

索引：通过保存一些额外的元数据来帮助查询的。

索引是从主数据衍生出来的额外结构，在写入数据时需要同步更新索引。

### 散列索引

以键值对数据为例，散列索引的结构如下：

![HashIndex Structure](/assets/img/DDIA_HashIndex_Structure.png)

- 主数据：存储在一个仅追加的文件中
- 索引：内存中的散列表，包含所有键及其在数据文件中的字节偏移量的映射

写入数据时，先追加写到文件，再更新内存中的散列表。

查询数据时，先查询散列表得到数据文件中的偏移量，再从文件中读取值。

Riak中的默认存储引擎Bitcask就使用了这种方法。

#### 如何避免磁盘空间被用完？

如果一直往文件尾部追加写，迟早会把磁盘空间用完。为了避免这种情况，可以把数据文件分为特定大小的段（segment），等到当前段文件增加到一定尺寸，就关闭当前段文件，并开始写入一个新的段文件。

然后，对之前的段文件进行压缩（compaction），去除重复的键，只保留最新值。此外，压缩后的段会变得很小，因此可以在压缩的同时合并多个段。压缩与合并的过程如下：

![HashIndex CompactionAndMerging](/assets/img/DDIA_HashIndex_CompactionAndMerging.png)

压缩与合并在后台线程中进行，结果写入一个新的段文件。这个过程进行时，仍然使用旧的段文件来查询，完成后将转为使用新的段文件，然后删除旧的段文件。

#### 数据文件分段后，索引有什么改变？

每个段文件都有一个对应的内存散列表。查询时，按照时间从最近的散列表开始找，找不到再往前找。

压缩与合并时，新的段文件会对应一个新的散列表，完成时这个新的散列表也会替换旧的散列表。

#### 其他考虑

- 文件格式

  使用CSV格式可能需要转义，使用二进制格式会更简单并且更快：先存字符串的长度（字节数），再存字符串本身。

- 删除键值对
    
  当删除一个键值对时，在文件末尾追加一个特殊的删除标识（tombstone）。在压缩与合并段文件时，去除这个键的所有值。

- 数据写入一半时崩溃了怎么办？

   在文件中包含校验和，允许检测和忽略损坏的部分。

- 数据库重启后如何重建索引？

  数据库重启后，内存中的散列表就没了。如果重新从段文件中重建散列表，可能需要很长时间。所以Bitcask会把散列表的快照存储在硬盘上，从而加快恢复散列表。

- 并发控制
  
  由于段文件是仅追加的，所以通常只有一个写入线程。由于段文件中已有的内容是不可变的，所以可以被多个线程同时读取。

#### 日志结构的优点

- 仅追加是顺序写入操作，比随机写入快很多。
- 由于段文件是仅追加的，已有内容不可变，因此并发和崩溃恢复都比较简单。
- 对旧段的压缩与合并可以避免数据文件的碎片化问题。

#### 散列索引的缺点

- 内存散列表必须存放所有键，如果键很多的话就不行了。
- 范围查询的效率不高。如果要查询0到100之间的所有键，必须单独查找每个键。

### LSM树

以键值对数据为例，LSM树（Log-Structured Merge-Tree，日志结构合并树）的结构如下：

![LSM Structure](/assets/img/DDIA_LSM_Structure.png)

- 每个段文件是一个SSTable（Sorted String Tables，排序字符串表）。
- 每个段文件中，键值对的序列按键排序，并且每个键只出现一次。
- 每个段文件对应一个内存中的索引，这个索引也是按键排序的，并且是稀疏的（几KB的段文件有一个键就足够了）。

假设要在图示段文件中查询键`handiwork`，先查找内存中的索引，发现它在`handbag`和`handsome`之间，于是从`handbag`的偏移位置开始扫描，直到`handsome`的偏移位置。如果扫描完了都没找到，就表示这个键在这个段中不存在。

由于查询时必须扫描完范围内所有的键值对，所以可以把这些键值对分组为块（block），对块进行压缩后再写入磁盘。这样既可以节省磁盘空间，又可以减少对I/O带宽的使用。

#### 怎么合并段文件？

如下图所示，合并段文件的过程采用类似归并排序的思想，查看每个段文件中的第一个键，复制最小的键到输出文件，不断重复这个步骤。这样可以保证新的段文件也是按键排序的。

![LSM CompactionAndMerging](/assets/img/DDIA_LSM_CompactionAndMerging.png)

如果几个段文件中有相同的键，根据段文件的创建顺序，保留最近的段文件中的值。

#### 怎么保证段文件是按键排序的？

虽然可以直接在硬盘上维护有序结构，但是在内存中维护会简单得多。LSM树使用内存表（memtable）来维护有序结构，内存表的数据结构使用平衡的二叉搜索树（如红黑树、AVL树），可以按任意顺序写入键，然后按排序顺序读取它们。

写入数据时，把数据添加到内存表中。当内存表大于某个阈值（通常为几MB）时，把它按排序顺序写入一个新的段文件。此时，新的写入在一个新的内存表中继续。

读取数据时，首先查找内存表，如果没有再去查找最近的段文件，如果还没有就查找更早的段文件，以此类推。

在后台运行一个线程，不断合并段文件。

#### 崩溃恢复

如果数据库崩溃了，内存表中的数据还没来得及写入段文件，那这些数据就丢失了。

为了避免这种情况，对于每个内存表，可以在硬盘上保存一个相应的日志。每次写入数据时，同时追加写到这个日志上。把内存表写入段文件后，相应的日志文件就可以删掉了。

> Q: 先写内存表，还是先写日志呢？

一旦数据库崩溃了，就利用这个日志来恢复内存表。

#### 有哪些基于LSM树的数据库？

基于LSM树的数据库包括：LevelDB、RocksDB、Cassandra和HBase。

#### 其他考虑

- 使用布隆过滤器（Bloom filters）来优化对不存在的键的访问

  对于不存在的键，需要查找内存表和所有段文件，才能确定键不存在。使用布隆过滤器可以快速检查数据库中是否存在某个键，从而避免大量磁盘读取操作。

- 合并段文件的策略
  - size-tiered：HBase、Canssandra
  - leveled compaction：LevelDB、RocksDB、Cassandra

#### LSM树的优点

- 索引是稀疏的，所以即使数据集比内存大得多，仍然可以正常工作
- 因为数据是按键排序的，所以可以高效地执行范围查询
- 因为硬盘写入是连续地，所以可以支持非常高的写入吞吐量

## 面向页面（page-oriented）的存储引擎

### B树

以键值对数据为例，B树的结构如下：

![BTree Structure](/assets/img/DDIA_BTree_Structure.png)

- 将数据库分为固定大小的分页（page），每个页面大小通常为4KB或更大，一次只能读取或写入一个页面。
- 每个页面中包含几个键和对子页面的引用。其中，键是排序的，引用的子页面包含相邻两个键范围的键。
- 叶子页面（leaf page）要么直接包含每个键的值，要么包含对值的引用。

查询一个键时，从根页面开始，根据键的大小逐层向下查询子页面，直到叶子页面。

修改一个键时，先找到包含该键的叶子页面，然后更改这个叶子页面并写回硬盘。

增加一个键时，先找到范围包含这个键的叶子页面，然后把键添加到这个叶子页面。如下所示，如果这个页面满了，就需要把它拆分成两个新的页面，并更新父页面来反映新的键范围分区。

![BTree Structure](/assets/img/DDIA_BTree_AddNewKey.png)

>Q：删除一个键的操作？

#### 分支因子

分支因子（branching factor）：一个页面对子页面的引用的数量。

分支因子的大小主要取决于：
- 页面的大小
- 存储键和子页面引用所需的空间

分支因子通常是几百。

#### B树的深度

具有`n`个键的B树深度为`O(logn)`。

通常B树的深度为3~4层，所以查询会比较快。

假设一个B树有4层，每个页面是4KB，分支因子是500，那它可以存储的数据量为：

```
h = 4
m = 500
s = 4KB

m^(h-1) * s = 500^3 * 4KB = 500GB
```

#### 崩溃恢复

B树的写操作需要修改页面，有时候还需要修改多个页面，一旦中途崩溃了，就会导致数据不一致。

为了处理异常崩溃的情况，可以有两种策略：

- 预写式日志（WAL，write-ahead log）：增加一个磁盘上的预写式日志，每个写操作都会先追加写到这个日志，再修改B树。发生崩溃后，可以用这个日志恢复B树。

- 写时复制：把修改后的页面写入新的位置。

#### 并发控制

如果有多个线程同时访问B树，需要并发控制，否则不同线程会看到不一致的数据。可以使用锁存器（latch）。

>Q：具体一点？

#### 其他考虑

- 不存储整个键，缩短其大小，从而节省空间
- 尽量让叶子页面按键的顺序存储在磁盘上，从而优化那些需要扫描多个键范围的查询
- 添加额外的指针，如每个叶子页面可以引用左右兄弟页面，方便对键按顺序扫描

### 比较LSM树和B树

- B树的实现比LSM树的实现更成熟
- LSM树的写入速度更快，B树的读取速度更快

#### 写入放大（write amplification）

写入放大：在数据库的生命周期中，每笔数据对磁盘的多次写入。

写入放大会导致的问题：
- 降低写入吞吐量（可用硬盘带宽内能处理的每秒写入次数）
- 固态硬盘的闪存寿命在覆写一定次数后耗尽

LSM树的写入放大更低，并且是顺序写入而不是随机写入，所以LSM树的写入吞吐量更高。

#### 存储开销

LSM树的存储开销更小，因为：
- LSM树可以被压缩得更好。
- LSM树的硬盘空间碎片化（fragmentation）更少：B树的页面中可能有没被使用的空间，而LSM树会定期合并SSTable来去除碎片。

#### 压缩的影响

LSM树需要在后台压缩SSTable，这会导致：
- 硬盘带宽需要在正常写入和后台压缩操作之间共享，数据库越大，后台压缩所需的硬盘带宽就越大。
- 硬盘带宽有限，正常的写入操作可能需要等待压缩完成，从而导致较高的P99响应时间。
- 当写入吞吐量很高时，压缩速率可能跟不上写入速率，导致占用的磁盘空间越来越多，读取速度越来越慢（需要检查更多段文件）。

## 其他索引结构

### 聚集索引 VS. 非聚集索引

定义：
- 非聚集索引

  索引中存储对数据的引用，数据存储的地方被称为堆文件（heap file）。

- 聚集索引

  索引中存储所有行数据。

- 覆盖索引（covering index）

  两者折衷，索引中存储一部分列。

优缺点对比：
- 存储空间

  非聚集索引不需要复制数据，比较节省存储空间。聚集索引和覆盖索引需要额外的存储空间。

- 写入开销

  对于非聚集索引，更新值的时候，如果新值的大小可以存储在旧值的位置，就只需要覆盖旧值，索引不用做任何改变。如果新值更大，就需要存储到堆中的新位置，然后要么更新索引中的位置引用，要么在旧堆位置保存一个转发指针。

  对于聚集索引和覆盖索引，更新值的的时候，既需要更新主数据，也需要更新索引中的数据。

- 读取速度

  非聚集索引需要从索引到堆文件的额外跳跃，读取速度更慢。聚集索引和覆盖索引的读取速度更快。

### 多列索引

多列索引：索引的键为多个字段的组合，字段的连接顺序是固定的。

多列索引包括：
- 连接索引（concatenated index）：将一列的值追加到另一列后面
- 多维索引（multi-dimensional index）：使用特殊的空间索引，如R树，可以处理地理空间数据

### 内存数据库

LSM树和B树都是基于磁盘的存储引擎。随着RAM变得越来越便宜，对于数据集不是特别大的情况，可以把数据全部存储在内存中。

有些内存数据库保证持久性，包括：
- VoltDB
- MemSQL 
- Oracle TimesTen 
- RAM Cloud 

它们要么通过特殊的硬件（如电池供电的RAM），要么通过把更新和快照写入硬盘日志来实现持久化。

有些内存数据库只有较弱的持久性，包括：
- Redis 
- Couchbase

它们通过异步写入硬盘来实现持久化。

内存数据库的优点：
- 不需要从硬盘读取（内存足够大的话，基于硬盘的存储引擎也可以不读硬盘，因为操作系统会在内存中缓存最近使用的硬盘块）
- 省去了将内存数据结构编码为硬盘数据结构的开销
- 提供了更多的数据模型，如优先级队列和集合等

## 数据仓库

### OLTP VS. OLAP

根据用途，数据库可以分为两类：
- 在线事务处理（OLTP, OnLine Transaction Processing）：用于处理业务数据
- 在线分析处理（OLAP, OnLine Analytic Processing）：用于数据分析

两者的比较：

| 属性 | OLTP | OLAP |
| --- | --- | --- |
| 主要读取模式 |	查询少量记录，按键读取 |	在大批量记录上聚合 |
| 主要写入模式 |	随机访问，写入要求低延时 |	批量导入（ETL）或者事件流 |
| 主要用户 |	终端用户，通过 Web 应用 |	内部数据分析师，用于决策支持 |
| 处理的数据 |	数据的最新状态（当前时间点 | 随时间推移的历史事件 |
| 数据集尺寸 |	GB ~ TB |	TB ~ PB |

最开始的时候，OLTP和OLAP是相同的数据库。这样做的问题在于，OLTP数据库对业务运作至关重要，通常要求高可用和低延迟，如果在OLTP数据库上进行数据分析，开销很大，会损害正在执行的事务的性能。

因此，OLAP开始使用单独的数据库——数据仓库（data warehouse）。

### ETL过程

数据仓库包含公司内各种OLTP数据库的只读数据副本。如下图所示，把数据从OLTP存入数据仓库的过程包括：抽取-转换-加载（ETL）。

![DataWarehouse ETL](/assets/img/DDIA_DataWarehouse_ETL.png)

- 抽取：通过定期的数据转储或连续的更新流，从OLTP数据库提取数据
- 转换：把数据转换成适合分析的模式
- 加载：把数据清理并加载到数据仓库中

### 产品

数据仓库产品有：
- 付费：Teradata、Vertica、SAP HANA、ParAccel 
- 开源：Apache Hive、Spark SQL、Cloudera Impala、Facebook Presto、Apache Tajo、Apache Drill

### 数据模型

数据仓库的模型主要有两种：
- 星型
- 雪花型

#### 星型模式

星型模式的示例如下：

![DataWarehouse Star](/assets/img/DDIA_DataWarehouse_Star.png)

- 事实表：表中每一行代表在特定时间发生的事件，如图中的`fact_sales`表。
- 维度表：表示事件发生的时间、地点、对象、内容、方式、原因等，如图中的`dim_product`表。

事实表在中间，被维度表包围，它们之间的连接就像星星的光芒，所以称为星型模式。

#### 雪花型模式

雪花型模式是星型模式的变体，它会把维度进一步分解为子维度。例如，把品牌和产品类别作为产品的子维度。

#### 星型 VS. 雪花型

雪花型模式比星型模式更规范，但星型模式使用更简单。

### 列式存储

事实表可能有万亿行和几PB的数据，而维度表通常小得多（几百万行），所以主要关注事实表的存储。

事实表通常非常宽，超过100列，甚至可能有几百列。然而，典型的数据仓库查询一次只会访问其中4个或5个列。

大多数OLTP数据库都采用行存储：一行中的所有值都相邻存储。如果数据仓库也采用行存储，需要将整个行（超过100个属性）从硬盘加载到内存中，解析并过滤。这个操作会很耗时。

列式存储：一列中的所有值都相邻存储。查询时，只需要读取和解析查询中使用的那些列，从而节省大量时间。

>Q: 怎么把不同列组合成一行？
>
>A: 行由列中的位置标识，一列中的第k项与另一列中的第k项都属于同一行。

#### 列压缩

可以通过压缩每一列的数据来降低对硬盘吞吐量的需求。由于每一列的值都相当重复，所以列式存储很适合压缩。

位图编码是数据仓库中特别有效的一种压缩技术，如下所示：

![DataWarehouse Bitmap](/assets/img/DDIA_DataWarehouse_Bitmap.png)

- 位图编码：一列中每一个可能的值被编码成一个位图
- 游程编码（run-length encoding）：如果大部分位图是稀疏的，可以把位图再进行游程编码，进一步压缩

位图编码不仅可以压缩数据，还可以快速定位记录，从而提高查询效率。

>Q: 位图编码怎么快速定位记录？

#### 对列数据进行排序

在列式存储中，每一列的数据是什么顺序呢？最简单的方式是按插入顺序来存储，这样插入一个新行只需要追加到每个列文件。

另一种方式是对列数据进行排序，排序需要对所有的列一起进行，保证一列中的第k项和另一列中的第k项属于同一行。

例如，可以先对第一列（如`date_key`）进行排序，对于第一列中具有相同值的情况，再按第二列（如`product_sk`）进行排序，以此类推。

排序的好处主要有两个：
- 用作索引，提高查询效率。
- 帮助压缩列：排序后，连续的值都在一起，可以用简单的游程编码进行压缩。

在数据仓库Vertica中，数据的不同备份采用了不同的排序顺序，可以在查询时调用最适合的备份。

#### 写入列式存储

对于排序的列式存储，如果想插入一行，就需要在所有列中都插入一个值。如果采用采用类似B树的就地更新方式，就需要重写所有的列文件，代价在大。

数据仓库Vertica采用了类似LSM树的更新方式：把所有的写操作都先写入到内存中，当积累到一定量时再与磁盘上的列文件合并，然后批量写入新文件。

采用这种更新方式，查询时需要同时检查内存中的最近写入和磁盘上的列数据，然后把两者的结果合并起来。

#### 物化视图和数据立方体

数据仓库的查询通常涉及聚合函数，如COUNT、SUM等。如果相同的聚合被许多不同的查询使用，那么每次都通过原始数据来处理可能太浪费了，可以把一些使用最频繁的聚合结果缓存起来，提高查询效率。

物化视图（Materialized View）就是一种对聚合结果进行缓存的方式，它会把聚合结果写入硬盘。当数据发生变化时，物化视图也需要更新，因此物化视图会增加写入成本。

物化视图 VS. 虚拟视图：
- 物化视图：查询结果的实际副本，会写入硬盘。
- 虚拟视图：编写查询的捷径，需要展开到视图的底层查询，然后执行。

数据立方体是物化视图的常见特例，如下图所示：

![DataWarehouse DataCube](/assets/img/DDIA_DataWarehouse_DataCube.png)

- 按维度对数据进行聚合
- 优点：让一些聚合查询变得非常快，因为结果已经预先计算了。
- 缺点：不像查询原始数据那样灵活。
