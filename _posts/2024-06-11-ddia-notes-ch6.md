---
layout: post
title: DDIA阅读笔记(6)：分布式数据系统的分区
date: 2024-06-11 +0800
tags: [系统设计, DDIA]
categories: [系统设计]
---

分区（partition），也称为分片（sharding），是一种有意把大型数据库分解成小型数据库的方式，有利于提高可伸缩性。

## 分区与复制

分区与复制通常会结合使用，使每个分区的副本存储在多个节点上，提高可用性。

如下所示，一个节点可能存储多个分区，它可能充当某些分区的主库和另一些分区的从库。

![DistributedData CombinePartitionAndReplication](/assets/img/DDIA_DistributedData_CombinePartitionAndReplication.png)

## 键值数据的分区

分区的目标：把数据和查询负载均匀分布在各个节点上。

偏斜（skew）：分区不均衡，导致一些分区比别的分区有更多的数据和查询。

热点（hot spot）：分区不均衡导致的高负载的分区。

对于键值数据，有两种方法可以进行分区：

- 根据键的范围分区
- 根据键的散列分区

### 根据键的范围分区

这种方法为每个分区指定一块连续的键范围。在读写数据时，只要确定了哪个分区包含某个值，再确定分区所在的节点，就可以去相应的节点发出请求了。

为了让数据均匀分布，分区边界需要仔细选择。在常见的数据库产品中，分区边界可以由管理员手动选择，也可以由数据库自动选择。

这种方法的好处是，在每个分区中，可以按照顺序保存键，从而方便进行范围扫描。

这种方法的坏处是，某些特定的访问模式会导致热点。例如，如果主键是时间戳，每个分区是一天的数据，那么当天的数据都会写入同一个分区中。

### 根据键的散列分区

这种方法使用一个散列函数计算键的哈希值，然后为每个分区指定一个散列范围（而不是键的范围）。在读写数据时，先计算键的哈希值，就可以确定相应的分区了。

为了让数据均匀分布，需要选择一个合适的散列函数。分区边界可以是均匀间隔的，也可以是伪随机选择的（一致性哈希，consistent hashing）。

这种方法的好处是，相比于键范围分区，可以让数据分布更均匀，减少热点。

这种方法的坏处是，键不是按照顺序保存的，所以范围查询比较麻烦，需要查询所有的分区。

### 热点消除

热点是无法完全避免的，例如，在极端情况下，所有读写操作都是针对同一个键的，那所有请求都会被路由到同一个分区。

数据库无法自动消除这种热点，需要应用程序采取一些措施。例如，如果一个主键太火爆了，就在主键的末尾添加一个随机数，让数据存储到不同的分区中。读取时，需要从所有相应分区中读取并合并。

## 分区与次级索引

次级索引（secondary index）通常并不能唯一标识记录，而是用来搜索包含特定值的记录，例如：查找所有红色的车辆。

在基于主键进行键值分区后，如果要再对次级索引进行分区，有两种方式：

- 基于文档（document-based）的分区
- 基于关键词（term-based）的分区

### 基于文档的分区

如下所示，每个分区的次级索引是独立的，只覆盖该分区中的数据。写入数据时，只需要处理该数据对应的分区。读取数据时，需要查询所有分区并合并，称为分散/聚集（scatter/gather）。

文档分区索引也称为本地索引，它被广泛使用于MongoDB、Riak、Cassandra、ElasticSearch等数据库中。

![DistributedData SecondaryKeyDocumentBasedPartition](/assets/img/DDIA_DistributedData_SecondaryKeyDocumentBasedPartition.png)

### 基于关键词的分区

如下所示，关键词分区索引是一种全局索引，覆盖了所有分区的数据，只不过这个全局索引会被分区。和键值数据的分区一样，关键词分区可以根据关键词本身或其散列进行。

相比于文档分区，关键词分区的优点在于，读取时只需要向包含关键词的那个分区发出请求，效率更高。缺点在于，写入时需要更新索引所在的多个分区，比较复杂，并且速度比较慢。

在实践中，对关键词分区索引的更新通常是异步的。

![DistributedData SecondaryKeyTermBasedPartition](/assets/img/DDIA_DistributedData_SecondaryKeyTermBasedPartition.png)

## 分区再平衡

再平衡（rebalancing）：将负载（数据存储、读取和写入请求）从一个节点向另一个节点移动的过程。

下列情况需要再平衡：
- 查询数据量增加，需要添加更多的CPU来处理。
- 数据集变大，需要添加更多的磁盘和RAM来存储。
- 机器出现故障，需要切换。

再平衡需要满足以下要求：
- 再平衡之后，负载应该在集群中的节点之间公平地共享。
- 再平衡发生时，数据库应该继续接受读取和写入。
- 节点之间只移动必须的数据，从而快速再平衡，并减少网络和磁盘I/O。

再平衡策略包括：
- 模N
- 固定数量的分区
- 动态分区
- 按节点比例分区

### 反面教材：模N

模N是一种把值进行散列的简单方法，但是它的问题是，如果N发生变化，大多数键都需要从一个节点移动到另一个节点，导致再平衡的成本太高。

### 固定数量的分区

如下所示，使分区数大于节点数，并为每个节点分配多个分区。当集群中添加了一个新节点时，从每个节点移动一些分区到新节点；当集群中删除一个节点时，把这个节点的分区移动到剩余的每个节点。

![DistributedData Rebalancing](/assets/img/DDIA_DistributedData_Rebalancing.png)

在建立数据库时确定分区的数量，之后不再改变。需要选择合适的分区数量，分区太多会增加管理开销，分区太少会导致每个分区太大，再平衡代价太大。

为了解决集群中硬件不匹配的问题，可以为更强大的节点分配更多的分区，从而让这些节点承担更多的负载。

### 动态分区

给分区设置大小限制，如果分区太大了，就拆分成两个分区，如果分区太小了，就和相邻分区合并。

为每个节点分配多个分区，大型分区拆分后，可以把其中一半转移到另一个节点，从而平衡负载。

动态分区的优点是，分区数量适应数据量。如果数据量小，只需要少量的分区，开销很小；如果数据量大，分区也会变多，而每个分区的大小是有限制的，避免了过大的分区带来的再平衡成本。

### 按节点比例分区

每个节点具有固定数量的分区，如果节点数不变，每个分区的大小随数据集成比例地增长。

如果一个新节点加入集群，随机选择固定数量的现有分区进行拆分，一半留在原地，一半移动到新节点。随机化可能会产生不公平的分割，有些数据库会引入其他再平衡的算法来避免。

### 自动还是手动再平衡？

自动再平衡很方便，但是不可预测。再平衡是一个昂贵的操作，对节点和网络都会造成额外的负担，如果没做好，可能会导致级联失败。

在实践中，会在自动和手动之间权衡。由数据库自动生成建议，但需要管理员提交才能生效。这样虽然比自动慢，但是可以防止意外。

## 请求路由

如果客户端想要读写一个数据，怎么知道要连接到哪个节点呢？这个问题可以概括为服务发现（service discovery），它不仅限于数据库，任何通过网络访问的软件都有这个问题。

### 如何把请求路由到正确的节点？

如下所示，把请求路由到正确的节点有三种方式：
1. 通过循环策略的负载均衡（Round-Robin Load Balancer），把客户端请求发送到任何节点。如果该节点有请求的分区，就直接处理请求，否则把请求转发到合适的节点。
2. 把客户端请求发送到路由层，再由路由层转发到合适的节点。路由层本身不处理请求，只负责负载均衡。
3. 客户端知道分区和节点的分配，可以直接连接到合适的节点。

![DistributedData RequestRouting](/assets/img/DDIA_DistributedData_RequestRouting.png)

以上三种方式中，负责路由决策的组件分别是节点、路由层或客户端。

### 负责路由决策的组件怎么知道分区和节点的分配呢？

如下所示，许多分布式数据系统都依赖一个独立的协调服务（如ZooKeeper）来跟踪集群元数据。

![DistributedData ZooKeeperTrackPartitionAndNode](/assets/img/DDIA_DistributedData_ZooKeeperTrackPartitionAndNode.png)

- 每个节点在ZooKeeper中注册自己，ZooKeeper维护分区到节点的映射。
- 负责路由决策的组件在ZooKeeper中订阅消息，只要分区分配发生了变化，或者集群中添加或删除了一个节点，ZooKeeper就会通知订阅者，使路由信息保持最新状态。

另外，有些分布式数据系统会在节点之间使用流言协议（gossip protocol），传播集群状态的变化。这种方式增加了节点的复杂性，但是避免了对ZooKeeper这样的外部协调服务的依赖。
